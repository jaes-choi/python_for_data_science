# 참고: hyperparameters에 의한 최적화
from sklearn.model_selection import GridSearchCV

param_grid = [
    {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]},
  ]

clf = LogisticRegression(solver='liblinear', multi_class='auto')
grid_search = GridSearchCV(clf, param_grid, cv=5, # n_jobs=-1,
                           scoring='accuracy', verbose=2, return_train_score=True)
grid_search.fit(X_train, y_train)

y_pred = grid_search.predict(X_test)

print(grid_search)
print(metrics.classification_report(y_test, y_pred))
print(metrics.confusion_matrix(y_test, y_pred))
print('accuracy is',metrics.accuracy_score(y_pred,y_test))

# 참고: RandomizedSearchCV

clfs = []

# LogisticRegression
from sklearn.linear_model import LogisticRegression
clfs.append(LogisticRegression(solver='liblinear', multi_class='auto'))

# Support Vector Machine 
from sklearn.svm import SVC
clfs.append(SVC(gamma=0.001))

# Naive Bayes
from sklearn.naive_bayes import GaussianNB
clfs.append(GaussianNB())

# K-Nearest Neighbours
from sklearn.neighbors import KNeighborsClassifier
clfs.append(KNeighborsClassifier(n_neighbors=8))

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
clfs.append(DecisionTreeClassifier())

# Random Forest
from sklearn.ensemble import RandomForestClassifier
clfs.append(RandomForestClassifier(max_depth=2, random_state=0))

# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
clfs.append(GradientBoostingClassifier(max_depth=2, random_state=0))

# Neural Network
from sklearn.neural_network import MLPClassifier
clfs.append(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=0))


--------------------------------------------------------------------

import json
import folium
loc_center = list(ta[['위도','경도']].mean())
m = folium.Map(location=loc_center, zoom_start=7)

# 한국 시도 경계 json 파일 읽기
# https://github.com/southkorea/southkorea-maps/blob/master/kostat/2013/json/skorea_provinces_geo_simple.json
geo_json = json.load(open('../data/skorea_provinces_geo_simple.json', encoding='utf-8'))

# ta_merged 파일 읽어오기
ta_merged = pd.read_pickle('../working/ta_merged.pkl')

# 대형사고율을 지도에 연결
folium.Choropleth(
    geo_data=geo_json,
    data=ta_merged,
    columns=['행정기관','사고/인구'], 
    key_on='feature.properties.name', 
    fill_color='Blues', # 'YlGn' 'Reds'
).add_to(m)
m

# folium

m.add_child(plugins.HeatMap(ta[['위도','경도']].values, radius=10)) 


m = folium.Map(location= loc_center, zoom_start=7, tiles='Stamen Toner')

# 지도 좌표에 원 그리기
for idx, row in ta.iterrows():
    folium.CircleMarker([row['위도'], row['경도']], 
                        radius=3, color='red', 
                        fill_color='red').add_to(m)

# subplot
# 하나에 4개 subplot 그려보기
colors=['red','blue']
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8,6))
axes[0,0].hist(y)
axes[0,1].hist(y, 20, color=colors, label=colors, stacked=True)
axes[0,1].legend()
axes[1,0].scatter(x,y1)
axes[1,0].scatter(x,y2)
axes[1,1].boxplot(y, labels=colors)
plt.show()

# 데이터 만들기 
np.random.seed(19961213)
N = 200
x = np.random.randn(N)
y1 = 10*x + 5*np.random.randn(N)
y2 = -5*x + 3*np.random.randn(N)+10

# pie
plt.pie(a, 
        autopct='%1.1f%%', 
        explode=[0,0,0,0,0.1],
        startangle=90, 
        shadow=True
       )
plt.show()

# groupby 결과를 dataframe 하나로 만들기
ta_agg = pd.DataFrame()

ta_agg['사망'] = ta.groupby('발생지시도')['사망자수'].sum()

ta_agg['중상'] = ta.groupby('발생지시도')['중상자수'].sum()

ta_agg['대형'] = ta.groupby('발생지시도')['대형사고'].sum()
ta_agg['대형사고율'] = ta.groupby('발생지시도')['대형사고'].mean()
ta_agg['사고건수'] = ta.groupby('발생지시도')['대형사고'].count()
ta_agg

# agg를 이용해서 한번에 적용
ta_agg = ta.groupby('발생지시도').agg({'사망자수':['sum','max'],'중상자수':'sum',
                                  '대형사고':['sum','mean','count']})
ta_agg.columns

ta_agg.columns = ['사망','사망_max','중상','대형','대형사고율','사고건수']
ta_agg


pd.DataFrame({'사망': ta.groupby('발생지시도')['사망자수'].sum(), 
              '중상': ta.groupby('발생지시도')['중상자수'].sum(), 
              '대형': ta.groupby('발생지시도')['대형사고'].sum(), 
              '대형사고율': ta.groupby('발생지시도')['대형사고'].mean(),
              '사고건수': ta.groupby('발생지시도')['대형사고'].count()})
              
              
# merge
ta_agg = pd.read_pickle('../working/ta_agg.pkl')

population.head(2)

ta_agg.head(2)

ta_agg2 = ta_agg.reset_index().rename(columns={'발생지시도':'시도'})

pop_merged =population.merge(ta_agg2, on='시도')

pop_merged['사고/인구'] = pop_merged['사고건수'] /pop_merged['총인구수']

pop_merged.to_pickle('../working/pop_merged.pkl')
